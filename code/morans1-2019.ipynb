{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5933bdc4-1be5-40f5-b7b7-8b676b78556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23244/3293934736.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['ALARM'] = pd.to_datetime(chunk_ca['ALARM'], format='%m%d%Y%H%M', errors='coerce')\n",
      "/tmp/ipykernel_23244/3293934736.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['ARRIVAL'] = pd.to_datetime(chunk_ca['ARRIVAL'], format='%m%d%Y%H%M', errors='coerce')\n",
      "/tmp/ipykernel_23244/3293934736.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['LU_CLEAR'] = pd.to_datetime(chunk_ca['LU_CLEAR'], format='%m%d%Y%H%M', errors='coerce')\n",
      "/tmp/ipykernel_23244/3293934736.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['ALARM'] = pd.to_datetime(chunk_ca['ALARM'], format='%m%d%Y%H%M', errors='coerce')\n",
      "/tmp/ipykernel_23244/3293934736.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['ARRIVAL'] = pd.to_datetime(chunk_ca['ARRIVAL'], format='%m%d%Y%H%M', errors='coerce')\n",
      "/tmp/ipykernel_23244/3293934736.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk_ca['LU_CLEAR'] = pd.to_datetime(chunk_ca['LU_CLEAR'], format='%m%d%Y%H%M', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to morans_data/2019/basicincident_processed_2019.csv\n",
      "Total rows: 28534829, CA rows: 3359106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 文件路径\n",
    "basicincident_file = os.path.join('DATA_nfirs2000-2021/NFIRS2019/basicincident.txt')\n",
    "processed_csv_file = os.path.join('morans_data/2019/basicincident_processed_2019.csv')\n",
    "\n",
    "# 处理并转换日期时间格式的函数\n",
    "def convert_datetime(dt_str):\n",
    "    return pd.to_datetime(dt_str, format='%m%d%Y%H%M', errors='coerce')\n",
    "\n",
    "# 处理basicincident.txt文件\n",
    "def process_basicincident(file_path, output_file):\n",
    "    chunk_size = 10000  # 每次读取的行数\n",
    "    total_rows = 0  # 记录总行数\n",
    "    ca_rows = 0  # 记录CA行数\n",
    "\n",
    "    # 初始化输出文件\n",
    "    first_chunk = True\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, delimiter='^', encoding='latin1', low_memory=False, chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "        # 过滤出加利福尼亚州的数据\n",
    "        chunk_ca = chunk[chunk['STATE'] == 'CA']\n",
    "        ca_rows += len(chunk_ca)\n",
    "\n",
    "        # 转换日期和时间格式\n",
    "        chunk_ca['ALARM'] = pd.to_datetime(chunk_ca['ALARM'], format='%m%d%Y%H%M', errors='coerce')\n",
    "        chunk_ca['ARRIVAL'] = pd.to_datetime(chunk_ca['ARRIVAL'], format='%m%d%Y%H%M', errors='coerce')\n",
    "        chunk_ca['LU_CLEAR'] = pd.to_datetime(chunk_ca['LU_CLEAR'], format='%m%d%Y%H%M', errors='coerce')\n",
    "\n",
    "        # 丢弃ALARM和ARRIVAL中为空的行\n",
    "        chunk_ca = chunk_ca.dropna(subset=['ALARM', 'ARRIVAL'])\n",
    "\n",
    "        # 确保列转换为日期时间类型\n",
    "        chunk_ca = chunk_ca[chunk_ca['ALARM'].notnull() & chunk_ca['ARRIVAL'].notnull()]\n",
    "        chunk_ca['ALARM'] = pd.to_datetime(chunk_ca['ALARM'], errors='coerce')\n",
    "        chunk_ca['ARRIVAL'] = pd.to_datetime(chunk_ca['ARRIVAL'], errors='coerce')\n",
    "        chunk_ca['LU_CLEAR'] = pd.to_datetime(chunk_ca['LU_CLEAR'], errors='coerce')\n",
    "\n",
    "        # 计算响应时间和解决火灾的时间\n",
    "        chunk_ca['RESPONSE_TIME'] = (chunk_ca['ARRIVAL'] - chunk_ca['ALARM']).dt.total_seconds() / 60  # 以分钟为单位\n",
    "        chunk_ca['FIRE_CLEARANCE_TIME'] = (chunk_ca['LU_CLEAR'] - chunk_ca['ARRIVAL']).dt.total_seconds() / 60  # 以分钟为单位\n",
    "\n",
    "        # 写入CSV文件\n",
    "        if first_chunk:\n",
    "            chunk_ca.to_csv(output_file, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk_ca.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "    print(f\"Total rows: {total_rows}, CA rows: {ca_rows}\")\n",
    "\n",
    "# 运行处理函数\n",
    "process_basicincident(basicincident_file, processed_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01743edb-0fdc-4aeb-b95a-c64980136438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to morans_data/2019/filtered_fire_data_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 文件路径\n",
    "processed_csv_file = 'morans_data/2019/basicincident_processed_2019.csv'\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "output_file = 'morans_data/2019/filtered_fire_data_2019.csv'\n",
    "\n",
    "# 读取 shapefile 数据\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 FDID 列为字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 空列表存储分块处理后的数据\n",
    "chunks = []\n",
    "chunk_size = 10000  # 每次处理的行数\n",
    "\n",
    "# 分块读取火灾数据并过滤加利福尼亚州的数据\n",
    "dtypes = {\n",
    "    'FDID': str,\n",
    "    'STATE': str,\n",
    "    'RESPONSE_TIME': float,\n",
    "    'FIRE_CLEARANCE_TIME': float,\n",
    "    'ALARM': str,\n",
    "    'ARRIVAL': str,\n",
    "    'LU_CLEAR': str\n",
    "}\n",
    "\n",
    "for chunk in pd.read_csv(processed_csv_file, chunksize=chunk_size, dtype=dtypes, low_memory=False):\n",
    "    chunk_ca = chunk[chunk['STATE'] == 'CA']\n",
    "    chunks.append(chunk_ca[['FDID', 'RESPONSE_TIME', 'FIRE_CLEARANCE_TIME']])\n",
    "\n",
    "# 合并所有分块\n",
    "fire_data_ca = pd.concat(chunks)\n",
    "\n",
    "# 保存处理后的数据\n",
    "fire_data_ca.to_csv(output_file, index=False)\n",
    "print(f\"Filtered data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bbf308-2065-4716-b794-52be4b7afa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/libpysal/cg/alpha_shapes.py:38: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "/opt/conda/lib/python3.11/site-packages/libpysal/cg/alpha_shapes.py:164: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "/opt/conda/lib/python3.11/site-packages/libpysal/cg/alpha_shapes.py:198: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "/opt/conda/lib/python3.11/site-packages/libpysal/cg/alpha_shapes.py:260: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of merged data: (2613313, 4)\n",
      "    FDID                                           geometry  RESPONSE_TIME  \\\n",
      "0  10005  MULTIPOLYGON (((34471.405 -135546.341, 34566.2...            3.0   \n",
      "1  10005  MULTIPOLYGON (((34471.405 -135546.341, 34566.2...            4.0   \n",
      "2  10005  MULTIPOLYGON (((34471.405 -135546.341, 34566.2...            4.0   \n",
      "3  10005  MULTIPOLYGON (((34471.405 -135546.341, 34566.2...            4.0   \n",
      "4  10005  MULTIPOLYGON (((34471.405 -135546.341, 34566.2...            2.0   \n",
      "\n",
      "   FIRE_CLEARANCE_TIME  \n",
      "0                  9.0  \n",
      "1                  9.0  \n",
      "2                  9.0  \n",
      "3                 14.0  \n",
      "4                  8.0  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 文件路径\n",
    "filtered_data_file = 'morans_data/2019/filtered_fire_data_2019.csv'\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "\n",
    "# 读取过滤后的火灾数据\n",
    "fire_data_ca = dd.read_csv(filtered_data_file)\n",
    "\n",
    "# 读取 shapefile 数据\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保火灾数据中的 FDID 列为字符串类型\n",
    "fire_data_ca['FDID'] = fire_data_ca['FDID'].astype(str)\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 将 fire_data_ca 转换为 pandas DataFrame\n",
    "fire_data_ca = fire_data_ca.compute()\n",
    "\n",
    "# 合并数据并仅保留必要的列\n",
    "merged = gdf[['FDID', 'geometry']].merge(fire_data_ca[['FDID', 'RESPONSE_TIME', 'FIRE_CLEARANCE_TIME']], on='FDID', how='inner')\n",
    "\n",
    "# 检查合并后的数据大小\n",
    "print(f\"Size of merged data: {merged.shape}\")\n",
    "\n",
    "# 检查合并后的数据\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cafd56f-45b2-4867-9fd8-cb3f1f11da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    County   FDID MACSID                               Name  \\\n",
      "0  ALAMEDA    nan    NaN               CAMP PARKS FIRE DEPT   \n",
      "1  ALAMEDA    nan    NaN  FAIRVIEW FIRE PROTECTION DISTRICT   \n",
      "2  ALAMEDA  01005    ALA                         ALAMEDA FD   \n",
      "3  ALAMEDA  01008    ACF                  ALAMEDA COUNTY FD   \n",
      "4  ALAMEDA  01010    ALB                  CITY OF ALBANY FD   \n",
      "\n",
      "              Address     City    Zip            FireChief           Phone  \\\n",
      "0                 NaN      NaN    NaN                  NaN             NaN   \n",
      "1                 NaN      NaN    NaN                  NaN  (510) 583-4940   \n",
      "2        1300 PARK ST  ALAMEDA  94501        NICHOLAS LUBY  (510) 337-2100   \n",
      "3      6363 CLARK AVE   DUBLIN  94568  WILLIAM L. MCDONALD  (925) 833-3473   \n",
      "4  1000 SAN PABLO AVE   ALBANY  94706          JAMES BOITO  (510) 528-5770   \n",
      "\n",
      "  Notes  LastUpdate                                            Website  \\\n",
      "0   NaN  2018-06-21                                             <Null>   \n",
      "1   NaN  2018-06-21              https://www.fairviewfiredistrict.org/   \n",
      "2   NaN  2018-06-21  https://www.alamedaca.gov/Departments/Fire-Dep...   \n",
      "3   NaN  2018-06-21                            https://fire.acgov.org/   \n",
      "4   NaN  2018-06-21  https://www.albanyca.org/departments/fire-depa...   \n",
      "\n",
      "  CALFIREUni                                           geometry  \n",
      "0        SCU  POLYGON ((-166377.337 -28813.876, -166264.746 ...  \n",
      "1        SCU  POLYGON ((-177810.883 -37513.784, -178272.697 ...  \n",
      "2        SCU  POLYGON ((-201596.954 -22267.837, -201603.038 ...  \n",
      "3        SCU  MULTIPOLYGON (((-175442.453 -53077.073, -17553...  \n",
      "4        SCU  POLYGON ((-201563.420 -10606.685, -201501.465 ...  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# 读取 Shapefile 数据\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 GeoDataFrame 中的 FDID 列为字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 检查读取的数据\n",
    "print(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af58e40-4395-454e-b6c5-06c73e1d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义一个处理数据块的函数\n",
    "def process_chunk(chunk):\n",
    "    chunk['FDID'] = chunk['FDID'].astype(str)\n",
    "\n",
    "    # 仅保留必要的列\n",
    "    chunk = chunk[['FDID', 'RESPONSE_TIME', 'FIRE_CLEARANCE_TIME']]\n",
    "    \n",
    "    # 合并数据\n",
    "    merged_chunk = gdf[['FDID', 'geometry']].merge(chunk, on='FDID', how='inner')\n",
    "    return merged_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e1ac11-d01a-4eec-b06a-0d621bd301e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `_to_string_dtype`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nTypeError(\"Cannot interpret '<geopandas.array.GeometryDtype object at 0xffff5d502dd0>' as a data type\")\n\nTraceback:\n---------\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6897, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6877, in _extract_meta\n    return tuple(_extract_meta(_x, nonempty) for _x in x)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6877, in <genexpr>\n    return tuple(_extract_meta(_x, nonempty) for _x in x)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6873, in _extract_meta\n    return x._meta_nonempty if nonempty else x._meta\n           ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 571, in _meta_nonempty\n    return meta_nonempty(self._meta)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/utils.py\", line 642, in __call__\n    return meth(arg, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py\", line 342, in meta_nonempty_dataframe\n    dt_s_dict[dt] = _nonempty_series(x.iloc[:, i], idx=idx)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py\", line 458, in _nonempty_series\n    data = np.array([entry, entry], dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py:193\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6897\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[0;32m-> 6897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6877\u001b[0m, in \u001b[0;36m_extract_meta\u001b[0;34m(x, nonempty)\u001b[0m\n\u001b[1;32m   6876\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 6877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_extract_meta(_x, nonempty) \u001b[38;5;28;01mfor\u001b[39;00m _x \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m   6878\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6877\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6876\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 6877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonempty\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _x \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m   6878\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6873\u001b[0m, in \u001b[0;36m_extract_meta\u001b[0;34m(x, nonempty)\u001b[0m\n\u001b[1;32m   6872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (Scalar, _Frame)):\n\u001b[0;32m-> 6873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta_nonempty\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m nonempty \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   6874\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:571\u001b[0m, in \u001b[0;36m_Frame._meta_nonempty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A non-empty version of `_meta` with fake data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeta_nonempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/utils.py:642\u001b[0m, in \u001b[0;36mDispatch.__call__\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py:342\u001b[0m, in \u001b[0;36mmeta_nonempty_dataframe\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dt_s_dict:\n\u001b[0;32m--> 342\u001b[0m     dt_s_dict[dt] \u001b[38;5;241m=\u001b[39m \u001b[43m_nonempty_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m data[i] \u001b[38;5;241m=\u001b[39m dt_s_dict[dt]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py:458\u001b[0m, in \u001b[0;36m_nonempty_series\u001b[0;34m(s, idx)\u001b[0m\n\u001b[1;32m    457\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _scalar_from_dtype(dtype)\n\u001b[0;32m--> 458\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry, entry], dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    460\u001b[0m out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(data, name\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mname, index\u001b[38;5;241m=\u001b[39midx)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<geopandas.array.GeometryDtype object at 0xffff5d502dd0>' as a data type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     28\u001b[0m meta \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame({\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFDID\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m: gpd\u001b[38;5;241m.\u001b[39mGeoSeries([], crs\u001b[38;5;241m=\u001b[39mgdf\u001b[38;5;241m.\u001b[39mcrs),\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRESPONSE_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIRE_CLEARANCE_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m })\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 使用 Dask 进行分块处理\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m fire_data_ca \u001b[38;5;241m=\u001b[39m \u001b[43mfire_data_ca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 将 Dask DataFrame 转换为 Pandas DataFrame 以进行后续处理\u001b[39;00m\n\u001b[1;32m     39\u001b[0m fire_data_pd \u001b[38;5;241m=\u001b[39m fire_data_ca\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:1048\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:7038\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   7033\u001b[0m     dsk \u001b[38;5;241m=\u001b[39m partitionwise_graph(\n\u001b[1;32m   7034\u001b[0m         func, name, \u001b[38;5;241m*\u001b[39margs2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs4, dependencies\u001b[38;5;241m=\u001b[39mdependencies\n\u001b[1;32m   7035\u001b[0m     )\n\u001b[1;32m   7037\u001b[0m graph \u001b[38;5;241m=\u001b[39m HighLevelGraph\u001b[38;5;241m.\u001b[39mfrom_collections(name, dsk, dependencies\u001b[38;5;241m=\u001b[39mdependencies)\n\u001b[0;32m-> 7038\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_dd_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivisions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:8219\u001b[0m, in \u001b[0;36mnew_dd_object\u001b[0;34m(dsk, name, meta, divisions, parent_meta)\u001b[0m\n\u001b[1;32m   8214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generic constructor for dask.dataframe objects.\u001b[39;00m\n\u001b[1;32m   8215\u001b[0m \n\u001b[1;32m   8216\u001b[0m \u001b[38;5;124;03mDecides the appropriate output class based on the type of `meta` provided.\u001b[39;00m\n\u001b[1;32m   8217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   8218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_parallel_type(meta):\n\u001b[0;32m-> 8219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_parallel_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_arraylike(meta) \u001b[38;5;129;01mand\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m   8221\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:4776\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, dsk, name, meta, divisions)\u001b[0m\n\u001b[1;32m   4775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dsk, name, meta, divisions):\n\u001b[0;32m-> 4776\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4778\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   4779\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[1;32m   4780\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4788\u001b[0m             },\n\u001b[1;32m   4789\u001b[0m         }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:445\u001b[0m, in \u001b[0;36m_Frame.__init__\u001b[0;34m(self, dsk, name, meta, divisions)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# this is an internal call, and if we enforce metadata,\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# it may interfere when reading csv with enforce=False\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pyarrow_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_pyarrow_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdask\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:1048\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6967\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6960\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   6961\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want the partitions to be aligned, and are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6962\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `map_partitions` directly, pass `align_dataframes=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6963\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   6965\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, _Frame)]\n\u001b[0;32m-> 6967\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43m_get_meta_map_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Scalar) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[1;32m   6969\u001b[0m     layer \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   6970\u001b[0m         (name, \u001b[38;5;241m0\u001b[39m): (\n\u001b[1;32m   6971\u001b[0m             apply,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6975\u001b[0m         )\n\u001b[1;32m   6976\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:7078\u001b[0m, in \u001b[0;36m_get_meta_map_partitions\u001b[0;34m(args, dfs, func, kwargs, meta, parent_meta)\u001b[0m\n\u001b[1;32m   7074\u001b[0m     parent_meta \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   7075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[1;32m   7076\u001b[0m     \u001b[38;5;66;03m# Use non-normalized kwargs here, as we want the real values (not\u001b[39;00m\n\u001b[1;32m   7077\u001b[0m     \u001b[38;5;66;03m# delayed values)\u001b[39;00m\n\u001b[0;32m-> 7078\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43m_emulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7079\u001b[0m     meta_is_emulated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   7080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6896\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_emulate\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   6892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6893\u001b[0m \u001b[38;5;124;03m    Apply a function using args / kwargs. If arguments contain dd.DataFrame /\u001b[39;00m\n\u001b[1;32m   6894\u001b[0m \u001b[38;5;124;03m    dd.Series, using internal cache (``_meta``) for calculation\u001b[39;00m\n\u001b[1;32m   6895\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6896\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraise_on_meta_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuncname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mudf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_numeric_only_deprecation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   6897\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py:214\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    205\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    213\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata inference failed in `_to_string_dtype`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nTypeError(\"Cannot interpret '<geopandas.array.GeometryDtype object at 0xffff5d502dd0>' as a data type\")\n\nTraceback:\n---------\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6897, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6877, in _extract_meta\n    return tuple(_extract_meta(_x, nonempty) for _x in x)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6877, in <genexpr>\n    return tuple(_extract_meta(_x, nonempty) for _x in x)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6873, in _extract_meta\n    return x._meta_nonempty if nonempty else x._meta\n           ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 571, in _meta_nonempty\n    return meta_nonempty(self._meta)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/utils.py\", line 642, in __call__\n    return meth(arg, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py\", line 342, in meta_nonempty_dataframe\n    dt_s_dict[dt] = _nonempty_series(x.iloc[:, i], idx=idx)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/backends.py\", line 458, in _nonempty_series\n    data = np.array([entry, entry], dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 文件路径\n",
    "filtered_data_file = 'morans_data/2019/filtered_fire_data_2019.csv'\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "\n",
    "# 读取 Shapefile 数据\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 使用 Dask 读取过滤后的火灾数据\n",
    "fire_data_ca = dd.read_csv(filtered_data_file, usecols=['FDID', 'RESPONSE_TIME', 'FIRE_CLEARANCE_TIME'])\n",
    "fire_data_ca['FDID'] = fire_data_ca['FDID'].astype(str)\n",
    "\n",
    "# 定义处理数据块的函数\n",
    "def process_chunk(chunk):\n",
    "    chunk['FDID'] = chunk['FDID'].astype(str)\n",
    "    merged_chunk = gdf[['FDID', 'geometry']].merge(chunk, on='FDID', how='inner')\n",
    "    return merged_chunk\n",
    "\n",
    "# 创建元数据\n",
    "meta = gpd.GeoDataFrame({\n",
    "    'FDID': pd.Series(dtype='str'),\n",
    "    'geometry': gpd.GeoSeries([], crs=gdf.crs),\n",
    "    'RESPONSE_TIME': pd.Series(dtype='float64'),\n",
    "    'FIRE_CLEARANCE_TIME': pd.Series(dtype='float64')\n",
    "})\n",
    "\n",
    "# 使用 Dask 进行分块处理\n",
    "fire_data_ca = fire_data_ca.map_partitions(process_chunk, meta=meta)\n",
    "\n",
    "# 将 Dask DataFrame 转换为 Pandas DataFrame 以进行后续处理\n",
    "fire_data_pd = fire_data_ca.compute()\n",
    "\n",
    "# 删除包含缺失值的样本\n",
    "fire_data_pd = fire_data_pd.dropna(subset=['RESPONSE_TIME', 'FIRE_CLEARANCE_TIME'])\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(fire_data_pd[['RESPONSE_TIME', 'FIRE_CLEARANCE_TIME']])\n",
    "\n",
    "# 使用 Elbow 方法选择 K 值\n",
    "inertia = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# 可视化 Elbow 方法\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertia, marker='o')\n",
    "plt.xlabel('Number of clusters, K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n",
    "\n",
    "# 应用 K-means 聚类 (假设选择的 K 值为 3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# 将聚类结果添加到原始数据中\n",
    "fire_data_pd['Cluster'] = clusters\n",
    "\n",
    "# 检查聚类结果\n",
    "print(fire_data_pd.head())\n",
    "\n",
    "# 可视化聚类结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=fire_data_pd['RESPONSE_TIME'], y=fire_data_pd['FIRE_CLEARANCE_TIME'], hue=fire_data_pd['Cluster'], palette='viridis')\n",
    "plt.title('K-means Clustering of Response Time and Fire Clearance Time (2019)')\n",
    "plt.xlabel('Response Time (minutes)')\n",
    "plt.ylabel('Fire Clearance Time (minutes)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d80fa3-1c7c-4218-adf2-773e4cc3eb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09594f1-b325-4436-ad81-89a23fa78382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================response time=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a6956-a8ea-4bb5-a651-4ebe0ec9cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# 使用Dask读取合并后的数据\n",
    "merged_data_file = 'morans_data/2019/merged_fire_data_2019.csv'\n",
    "merged = dd.read_csv(merged_data_file)\n",
    "\n",
    "# 确保Dask dataframe中FDID是字符串类型\n",
    "merged['FDID'] = merged['FDID'].astype(str)\n",
    "\n",
    "# 显示前几行，确保数据正确读取\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddaea3-c79e-4dce-98f4-e29824e1a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 FDID 生成一个索引\n",
    "fdid_unique = merged['FDID'].unique().compute()\n",
    "fdid_index = {fdid: idx for idx, fdid in enumerate(fdid_unique)}\n",
    "merged['FDID_index'] = merged['FDID'].map(fdid_index)\n",
    "\n",
    "# 显示前几行，确保索引正确生成\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ea81a-bbd3-4a32-864c-7ae60c50d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from libpysal.weights import DistanceBand\n",
    "from esda.moran import Moran\n",
    "\n",
    "# 定义计算 Moran's I 的函数\n",
    "def compute_moran(chunk):\n",
    "    fdid_index_values = chunk['FDID_index'].values\n",
    "    y_response = chunk['RESPONSE_TIME'].values\n",
    "    \n",
    "    # 创建权重矩阵\n",
    "    coords = np.array(list(enumerate(fdid_index_values)))\n",
    "    w = DistanceBand(coords, threshold=1.5, binary=True, silence_warnings=True)\n",
    "    \n",
    "    moran_response = Moran(y_response, w)\n",
    "    return moran_response.I, moran_response.p_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16058a8c-8736-481c-9679-dab559194d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分块计算 Moran's I\n",
    "results = merged.map_partitions(compute_moran, meta=('I', 'f8')).compute()\n",
    "\n",
    "# 显示计算结果\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7d16f-0df3-48d0-932e-e5a84bac579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 将结果转换为 DataFrame\n",
    "results_df = pd.DataFrame(results.tolist(), columns=['Moran_I', 'p_value'])\n",
    "\n",
    "# 添加 FDID 列\n",
    "results_df['FDID'] = merged['FDID'].unique().compute()\n",
    "\n",
    "# 保存结果到 CSV 文件\n",
    "results_df.to_csv('morans_data/2019/morans_i_results2019_rt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d669fb-aced-4c73-87cf-1da9097cbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算总体的 Moran's I 平均值\n",
    "average_moran_i = np.mean([res[0] for res in results])\n",
    "average_p_value = np.mean([res[1] for res in results])\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Average Response Time Moran's I: {average_moran_i}\")\n",
    "print(f\"Average Response Time P-value: {average_p_value}\")\n",
    "\n",
    "if average_p_value < 0.05:\n",
    "    print(\"存在显著的火灾响应时间空间自相关\")\n",
    "else:\n",
    "    print(\"没有显著的火灾响应时间空间自相关\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4e4a3-19ab-4c58-ac0f-19625681cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 Shapefile 文件\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 GeoDataFrame 中 FDID 是字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 检查并设置 CRS 为 EPSG:4326\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# 读取保存的 Moran's I 结果\n",
    "results_df = pd.read_csv('morans_data/2019/morans_i_results2019_rt.csv')\n",
    "\n",
    "# 过滤掉包含空白 FDID 的行\n",
    "results_df = results_df[results_df['FDID'].notna()]\n",
    "\n",
    "# 将 FDID 转换为整数类型并再转换为字符串类型\n",
    "results_df['FDID'] = results_df['FDID'].astype(int).astype(str)\n",
    "\n",
    "# 检查合并前的数据\n",
    "print(results_df.head())\n",
    "print(gdf.head())\n",
    "\n",
    "# 将 Moran's I 结果合并到 GeoDataFrame 中\n",
    "gdf = gdf.merge(results_df, on='FDID', how='left')\n",
    "\n",
    "# 检查合并后的数据\n",
    "print(gdf.head())\n",
    "\n",
    "# 绘制地图，展示 Moran's I 结果，设置缺失数据的颜色\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "gdf.plot(column='Moran_I', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, \n",
    "         missing_kwds={\"color\": \"lightgrey\", \"label\": \"No data\"})\n",
    "ax.set_title('Moran\\'s I of Fire RESPONSE Time by FDID')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e906e5b-98d2-4c9c-a302-ac74312c9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============='FIRE_CLEARANCE_TIME'==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ae5db-0f50-4bdf-9352-b41a42270c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# 使用Dask读取合并后的数据\n",
    "merged_data_file = 'morans_data/2019/merged_fire_data_2019.csv'\n",
    "merged = dd.read_csv(merged_data_file)\n",
    "\n",
    "# 确保Dask dataframe中FDID是字符串类型\n",
    "merged['FDID'] = merged['FDID'].astype(str)\n",
    "\n",
    "# 显示前几行，确保数据正确读取\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af14c98-b97f-46db-a791-895525be1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 FDID 生成一个索引\n",
    "fdid_unique = merged['FDID'].unique().compute()\n",
    "fdid_index = {fdid: idx for idx, fdid in enumerate(fdid_unique)}\n",
    "merged['FDID_index'] = merged['FDID'].map(fdid_index)\n",
    "\n",
    "# 显示前几行，确保索引正确生成\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b5d75-112f-460a-9d87-f11bc0bdd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from libpysal.weights import DistanceBand\n",
    "from esda.moran import Moran\n",
    "\n",
    "# 定义计算 Moran's I 的函数\n",
    "def compute_moran(chunk):\n",
    "    fdid_index_values = chunk['FDID_index'].values\n",
    "    y_response = chunk['FIRE_CLEARANCE_TIME'].values\n",
    "    \n",
    "    # 创建权重矩阵\n",
    "    coords = np.array(list(enumerate(fdid_index_values)))\n",
    "    w = DistanceBand(coords, threshold=1.5, binary=True, silence_warnings=True)\n",
    "    \n",
    "    moran_response = Moran(y_response, w)\n",
    "    return moran_response.I, moran_response.p_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ec5ed-77b6-4585-9bd6-fa1cf69bcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分块计算 Moran's I\n",
    "results = merged.map_partitions(compute_moran, meta=('I', 'f8')).compute()\n",
    "\n",
    "# 显示计算结果\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2687090-bda4-4b3f-b55e-be1565a3eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 将结果转换为 DataFrame\n",
    "results_df = pd.DataFrame(results.tolist(), columns=['Moran_I', 'p_value'])\n",
    "\n",
    "# 添加 FDID 列\n",
    "results_df['FDID'] = merged['FDID'].unique().compute()\n",
    "\n",
    "# 保存结果到 CSV 文件\n",
    "results_df.to_csv('morans_data/2019/morans_i_results2019_clear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11089d9-ce08-4305-b0cd-85918bc1d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉包含 nan 的结果\n",
    "filtered_results = [res for res in results if not np.isnan(res[0]) and not np.isnan(res[1])]\n",
    "\n",
    "# 检查是否有有效的结果\n",
    "if filtered_results:\n",
    "    # 计算总体的 Moran's I 平均值\n",
    "    average_moran_i = np.mean([res[0] for res in filtered_results])\n",
    "    average_p_value = np.mean([res[1] for res in filtered_results])\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Average Fire Clearance Time Moran's I: {average_moran_i}\")\n",
    "    print(f\"Average Fire Clearance Time P-value: {average_p_value}\")\n",
    "\n",
    "    if average_p_value < 0.05:\n",
    "        print(\"存在显著的火灾清除时间空间自相关\")\n",
    "    else:\n",
    "        print(\"没有显著的火灾清除时间空间自相关\")\n",
    "else:\n",
    "    print(\"所有分区的计算结果都包含 nan 值，无法计算总体的 Moran's I 平均值。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe770a3-b6e0-436f-af01-a38930173c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# 读取Shapefile文件\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保GeoDataFrame中FDID是字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 显示前几行，确保数据正确读取\n",
    "print(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c7e39-293d-4f8e-a3e2-755b7069f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights import DistanceBand\n",
    "from esda.moran import Moran\n",
    "\n",
    "# 使用 Dask 读取合并后的数据，这里包含response time\n",
    "merged_data_file = 'morans_data/2019/merged_fire_data_2019.csv'\n",
    "merged = dd.read_csv(merged_data_file)\n",
    "\n",
    "# 确保 Dask DataFrame 中 FDID 是字符串类型\n",
    "merged['FDID'] = merged['FDID'].astype(str)\n",
    "\n",
    "# 读取 Shapefile 文件\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 GeoDataFrame 中 FDID 是字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 检查并设置 CRS 为 EPSG:4326\n",
    "gdf = gdf.to_crs(epsg=4326)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c7e4c-b01c-4af8-8eb3-950dfdc5bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 Shapefile 文件\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 GeoDataFrame 中 FDID 是字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 检查并设置 CRS 为 EPSG:4326\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# 读取保存的 Moran's I 结果\n",
    "results_df = pd.read_csv('morans_data/2019/morans_i_results2019_clear.csv')\n",
    "\n",
    "# 过滤掉包含空白 FDID 的行\n",
    "results_df = results_df[results_df['FDID'].notna()]\n",
    "\n",
    "# 将 FDID 转换为整数类型并再转换为字符串类型\n",
    "results_df['FDID'] = results_df['FDID'].astype(int).astype(str)\n",
    "\n",
    "# 检查合并前的数据\n",
    "print(results_df.head())\n",
    "print(gdf.head())\n",
    "\n",
    "# 将 Moran's I 结果合并到 GeoDataFrame 中\n",
    "gdf = gdf.merge(results_df, on='FDID', how='left')\n",
    "\n",
    "# 检查合并后的数据\n",
    "print(gdf.head())\n",
    "\n",
    "# 绘制地图，展示 Moran's I 结果，设置缺失数据的颜色\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "gdf.plot(column='Moran_I', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, \n",
    "         missing_kwds={\"color\": \"lightgrey\", \"label\": \"No data\"})\n",
    "ax.set_title('Moran\\'s I of Fire Clearance Time by FDID')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7c935-e328-4a71-9931-39cd036e751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 Shapefile 文件\n",
    "shapefile_path = 'california_shapefile/FireDistricts24_1.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 确保 GeoDataFrame 中 FDID 是字符串类型\n",
    "gdf['FDID'] = gdf['FDID'].astype(str)\n",
    "\n",
    "# 检查并设置 CRS 为 EPSG:4326\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# 读取保存的 Moran's I 结果\n",
    "results_df = pd.read_csv('morans_data/2019/morans_i_results2019_clear.csv')\n",
    "\n",
    "# 过滤掉包含空白 FDID 的行\n",
    "results_df = results_df[results_df['FDID'].notna()]\n",
    "\n",
    "# 将 FDID 转换为整数类型并再转换为字符串类型\n",
    "results_df['FDID'] = results_df['FDID'].astype(int).astype(str)\n",
    "\n",
    "# 检查合并前的数据\n",
    "print(results_df.head())\n",
    "print(gdf.head())\n",
    "\n",
    "# 将 Moran's I 结果合并到 GeoDataFrame 中\n",
    "gdf = gdf.merge(results_df, on='FDID', how='left')\n",
    "\n",
    "# 检查合并后的数据\n",
    "print(gdf.head())\n",
    "\n",
    "# 绘制地图，展示 Moran's I 结果，设置缺失数据的颜色\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "gdf.plot(column='Moran_I', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, \n",
    "         missing_kwds={\"color\": \"lightgrey\", \"label\": \"No data\"})\n",
    "ax.set_title('Moran\\'s I of Fire Clearance Time by FDID')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8eb72-a207-4ac8-acd7-c2f6530ec8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a0bcd-409e-4f5b-9394-5151a607fb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c649247-90a6-42e3-bf65-251215a0428c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
