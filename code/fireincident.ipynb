{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b577c02-5820-4281-b932-d27fda4a0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c91b9c0-9516-4504-9db4-83b05380f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 远程文件的 URL\n",
    "url = 'https://fema.gov/about/reports-and-data/openfema/nfirs_fire_hazmat_pdr_2022.zip'\n",
    "# 本地保存的文件名\n",
    "local_zip_file = 'nfirs_fire_hazmat_pdr_2022.zip'\n",
    "# 第一级解压缩后的文件夹名\n",
    "first_extracted_folder = 'nfirs_fire_hazmat_pdr_2022'\n",
    "# 第二级解压缩后的文件夹名\n",
    "second_extracted_folder = os.path.join(first_extracted_folder, 'NFIRS_FIRES_2022_102623')\n",
    "# 需要读取的文件路径\n",
    "fireincident_file = 'NFIRS_FIRES_2022_102623/fireincident.txt'\n",
    "basicincident_file = 'NFIRS_FIRES_2022_102623/basicincident.txt'\n",
    "civiliancasualty_file = 'NFIRS_FIRES_2022_102623/civiliancasualty.txt'\n",
    "output_count_civiliancasualty = os.path.join(second_extracted_folder, 'count_civiliancasualty.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a934772-c4d4-4031-8a50-0075d563856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded nfirs_fire_hazmat_pdr_2022.zip\n"
     ]
    }
   ],
   "source": [
    "# 下载文件\n",
    "response = requests.get(url)\n",
    "with open(local_zip_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print(f\"Downloaded {local_zip_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9399aaaf-9d9f-4963-8213-59998fdfe5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to nfirs_fire_hazmat_pdr_2022\n"
     ]
    }
   ],
   "source": [
    "# 解压缩第一级文件\n",
    "with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(first_extracted_folder)\n",
    "print(f\"Extracted files to {first_extracted_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c064ad-d355-4272-8d58-39af98a15f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623\n"
     ]
    }
   ],
   "source": [
    "# 解压缩第二级文件\n",
    "second_zip_file = os.path.join(first_extracted_folder, 'nfirs_fire_hazmat_pdr_2022', 'NFIRS_FIRES_2022_102623.zip')\n",
    "with zipfile.ZipFile(second_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(second_extracted_folder)\n",
    "print(f\"Extracted files to {second_extracted_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec24fb49-0e5f-4a54-94c2-293cb2e2ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFIRS_FIRES_2022_102623/\n",
      "    incidentaddress.txt\n",
      "    merged_data.csv\n",
      "    fireincident.txt\n",
      "    merged_casualty_data.csv\n",
      "    civiliancasualty.txt\n",
      "    hazchem.txt\n",
      "    arson.txt\n",
      "    basicincident.txt\n",
      "    hazmat.txt\n",
      "    wildlands.txt\n",
      "    arsonjuvsub.txt\n",
      "    ems.txt\n",
      "    hazmatequipinvolved.txt\n",
      "    codelookup.txt\n",
      "    ffcasualty.txt\n",
      "    fdheader.txt\n",
      "    hazmobprop.txt\n",
      "    basicaid.txt\n",
      "    ffequipfail.txt\n",
      "    filtered_basicincident.csv\n",
      "    count_civiliancasualty.txt\n",
      "    arsonagencyreferal.txt\n"
     ]
    }
   ],
   "source": [
    "# 列出解压后的目录结构\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "list_files(second_extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4d756a-426b-4d61-a074-f9f1cfb90bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 洛杉矶县城市列表\n",
    "la_cities = [\n",
    "    \"Agoura Hills\", \"Alhambra\", \"Arcadia\", \"Artesia\", \"Avalon\", \"Azusa\", \"Baldwin Park\", \"Bell\", \"Bell Gardens\", \"Bellflower\",\n",
    "    \"Beverly Hills\", \"Bradbury\", \"Burbank\", \"Calabasas\", \"Carson\", \"Cerritos\", \"Claremont\", \"Commerce\", \"Compton\", \"Covina\",\n",
    "    \"Cudahy\", \"Culver City\", \"Diamond Bar\", \"Downey\", \"Duarte\", \"El Monte\", \"El Segundo\", \"Gardena\", \"Glendale\", \"Glendora\",\n",
    "    \"Hawaiian Gardens\", \"Hawthorne\", \"Hermosa Beach\", \"Hidden Hills\", \"Huntington Park\", \"Industry\", \"Inglewood\", \"Irwindale\",\n",
    "    \"La Cañada Flintridge\", \"La Habra Heights\", \"La Mirada\", \"La Puente\", \"La Verne\", \"Lakewood\", \"Lancaster\", \"Lawndale\",\n",
    "    \"Lomita\", \"Long Beach\", \"Los Angeles\", \"Lynwood\", \"Malibu\", \"Manhattan Beach\", \"Maywood\", \"Monrovia\", \"Montebello\",\n",
    "    \"Monterey Park\", \"Norwalk\", \"Palmdale\", \"Palos Verdes Estates\", \"Paramount\", \"Pasadena\", \"Pico Rivera\", \"Pomona\",\n",
    "    \"Rancho Palos Verdes\", \"Redondo Beach\", \"Rolling Hills\", \"Rolling Hills Estates\", \"Rosemead\", \"San Dimas\", \"San Fernando\",\n",
    "    \"San Gabriel\", \"San Marino\", \"Santa Clarita\", \"Santa Fe Springs\", \"Santa Monica\", \"Sierra Madre\", \"Signal Hill\",\n",
    "    \"South El Monte\", \"South Gate\", \"South Pasadena\", \"Temple City\", \"Torrance\", \"Vernon\", \"Walnut\", \"West Covina\",\n",
    "    \"West Hollywood\", \"Westlake Village\", \"Whittier\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1392766e-d17e-438e-90e6-3804be5e5d9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+------------+---------+----------+\n| Column     | Found   | Expected |\n+------------+---------+----------+\n| ACTIV_INJ  | object  | float64  |\n| AGE        | float64 | int64    |\n| BODY_PART  | object  | int64    |\n| CAUSE_INJ  | object  | int64    |\n| FDID       | object  | int64    |\n| GENDER     | float64 | int64    |\n| GEN_LOC_IN | object  | float64  |\n| LOC_INC    | object  | float64  |\n| PRIM_SYMP  | object  | float64  |\n| RACE       | object  | float64  |\n| SEV        | object  | int64    |\n| SPC_LOC_IN | object  | float64  |\n+------------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- ACTIV_INJ\n  ValueError(\"could not convert string to float: 'U'\")\n- BODY_PART\n  ValueError('cannot convert float NaN to integer')\n- CAUSE_INJ\n  ValueError('cannot convert float NaN to integer')\n- FDID\n  ValueError(\"invalid literal for int() with base 10: 'AA211'\")\n- GEN_LOC_IN\n  ValueError(\"could not convert string to float: 'U'\")\n- LOC_INC\n  ValueError(\"could not convert string to float: 'U'\")\n- PRIM_SYMP\n  ValueError(\"could not convert string to float: 'UU'\")\n- RACE\n  ValueError(\"could not convert string to float: 'U'\")\n- SEV\n  ValueError(\"invalid literal for int() with base 10: 'U'\")\n- SPC_LOC_IN\n  ValueError(\"could not convert string to float: 'UU'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'ACTIV_INJ': 'object',\n       'AGE': 'float64',\n       'BODY_PART': 'object',\n       'CAUSE_INJ': 'object',\n       'FDID': 'object',\n       'GENDER': 'float64',\n       'GEN_LOC_IN': 'object',\n       'LOC_INC': 'object',\n       'PRIM_SYMP': 'object',\n       'RACE': 'object',\n       'SEV': 'object',\n       'SPC_LOC_IN': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     civiliancasualty_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(civiliancasualty_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     civiliancasualty_count \u001b[38;5;241m=\u001b[39m \u001b[43mciviliancasualty_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINCIDENT_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcivil_num\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     civiliancasualty_count\u001b[38;5;241m.\u001b[39mto_csv(output_count_civiliancasualty, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_count_civiliancasualty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m(df, dtypes)\u001b[0m\n\u001b[1;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+------------+---------+----------+\n| Column     | Found   | Expected |\n+------------+---------+----------+\n| ACTIV_INJ  | object  | float64  |\n| AGE        | float64 | int64    |\n| BODY_PART  | object  | int64    |\n| CAUSE_INJ  | object  | int64    |\n| FDID       | object  | int64    |\n| GENDER     | float64 | int64    |\n| GEN_LOC_IN | object  | float64  |\n| LOC_INC    | object  | float64  |\n| PRIM_SYMP  | object  | float64  |\n| RACE       | object  | float64  |\n| SEV        | object  | int64    |\n| SPC_LOC_IN | object  | float64  |\n+------------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- ACTIV_INJ\n  ValueError(\"could not convert string to float: 'U'\")\n- BODY_PART\n  ValueError('cannot convert float NaN to integer')\n- CAUSE_INJ\n  ValueError('cannot convert float NaN to integer')\n- FDID\n  ValueError(\"invalid literal for int() with base 10: 'AA211'\")\n- GEN_LOC_IN\n  ValueError(\"could not convert string to float: 'U'\")\n- LOC_INC\n  ValueError(\"could not convert string to float: 'U'\")\n- PRIM_SYMP\n  ValueError(\"could not convert string to float: 'UU'\")\n- RACE\n  ValueError(\"could not convert string to float: 'U'\")\n- SEV\n  ValueError(\"invalid literal for int() with base 10: 'U'\")\n- SPC_LOC_IN\n  ValueError(\"could not convert string to float: 'UU'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'ACTIV_INJ': 'object',\n       'AGE': 'float64',\n       'BODY_PART': 'object',\n       'CAUSE_INJ': 'object',\n       'FDID': 'object',\n       'GENDER': 'float64',\n       'GEN_LOC_IN': 'object',\n       'LOC_INC': 'object',\n       'PRIM_SYMP': 'object',\n       'RACE': 'object',\n       'SEV': 'object',\n       'SPC_LOC_IN': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "# 读取并处理 civiliancasualty.txt 文件\n",
    "civiliancasualty_path = os.path.join(second_extracted_folder, 'civiliancasualty.txt')\n",
    "output_count_civiliancasualty = os.path.join(second_extracted_folder, 'count_civiliancasualty.txt')\n",
    "\n",
    "if os.path.exists(civiliancasualty_path):\n",
    "    try:\n",
    "        civiliancasualty_df = dd.read_csv(civiliancasualty_path, delimiter='^', encoding='latin1')\n",
    "        civiliancasualty_count = civiliancasualty_df.groupby('INCIDENT_KEY').size().compute().reset_index(name='civil_num')\n",
    "        civiliancasualty_count.to_csv(output_count_civiliancasualty, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Generated {output_count_civiliancasualty}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading the file with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file at path {civiliancasualty_path} does not exist.\")\n",
    "\n",
    "# 合并 count_civiliancasualty.txt, fireincident.txt, basicincident.txt 文件\n",
    "fireincident_path = os.path.join(second_extracted_folder, 'fireincident.txt')\n",
    "basicincident_path = os.path.join(second_extracted_folder, 'basicincident.txt')\n",
    "output_merged_file = os.path.join(second_extracted_folder, 'merged_data.csv')\n",
    "\n",
    "if os.path.exists(fireincident_path) and os.path.exists(basicincident_path):\n",
    "    try:\n",
    "        fireincident_df = dd.read_csv(fireincident_path, delimiter='^', encoding='latin1')\n",
    "        basicincident_df = dd.read_csv(basicincident_path, delimiter='^', encoding='latin1')\n",
    "        count_civiliancasualty_df = dd.read_csv(output_count_civiliancasualty, delimiter='^', encoding='latin1')\n",
    "\n",
    "        merged_df = dd.merge(fireincident_df, basicincident_df, on='INCIDENT_KEY', how='left')\n",
    "        merged_df = dd.merge(merged_df, count_civiliancasualty_df, on='INCIDENT_KEY', how='left')\n",
    "\n",
    "        merged_df.to_csv(output_merged_file, single_file=True, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Merged data saved to {output_merged_file}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading one of the files with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(\"Error: One of the required files does not exist.\")\n",
    "\n",
    "# 显示合并后的数据\n",
    "if os.path.exists(output_merged_file):\n",
    "    merged_df = pd.read_csv(output_merged_file, delimiter='^', encoding='latin1')\n",
    "    print(merged_df.head())\n",
    "else:\n",
    "    print(f\"Error: The file at path {output_merged_file} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fb2dc-3893-4fdb-9c56-74644de72640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并 count_civiliancasualty.txt, fireincident.txt, basicincident.txt 文件\n",
    "fireincident_path = os.path.join(second_extracted_folder, 'fireincident.txt')\n",
    "basicincident_path = os.path.join(second_extracted_folder, 'basicincident.txt')\n",
    "output_merged_file = os.path.join(second_extracted_folder, 'merged_data.csv')\n",
    "\n",
    "# 指定数据类型以避免混合类型警告\n",
    "dtype_dict = {\n",
    "    'INCIDENT_KEY': str,\n",
    "    # 可以根据需要添加更多的列及其类型\n",
    "}\n",
    "\n",
    "if os.path.exists(fireincident_path) and os.path.exists(basicincident_path):\n",
    "    try:\n",
    "        chunk_size = 10000  # 分批处理的行数\n",
    "        \n",
    "        merged_df = pd.DataFrame()\n",
    "        \n",
    "        fireincident_chunks = pd.read_csv(fireincident_path, delimiter='^', encoding='latin1', dtype=dtype_dict, chunksize=chunk_size, low_memory=False)\n",
    "        basicincident_chunks = pd.read_csv(basicincident_path, delimiter='^', encoding='latin1', dtype=dtype_dict, chunksize=chunk_size, low_memory=False)\n",
    "        count_civiliancasualty_df = pd.read_csv(output_count_civiliancasualty, delimiter='^', encoding='latin1', dtype=dtype_dict)\n",
    "\n",
    "        for fireincident_chunk in fireincident_chunks:\n",
    "            for basicincident_chunk in basicincident_chunks:\n",
    "                temp_df = fireincident_chunk.merge(basicincident_chunk, on='INCIDENT_KEY', how='left')\n",
    "                # 移除全为空的列\n",
    "                temp_df = temp_df.dropna(axis=1, how='all')\n",
    "                merged_df = pd.concat([merged_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        merged_df = merged_df.merge(count_civiliancasualty_df, on='INCIDENT_KEY', how='left')\n",
    "\n",
    "        merged_df.to_csv(output_merged_file, index=False, sep='^', encoding='latin1')\n",
    "        print(f\"Merged data saved to {output_merged_file}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading one of the files with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(\"Error: One of the required files does not exist.\")\n",
    "\n",
    "# 显示合并后的数据\n",
    "if os.path.exists(output_merged_file):\n",
    "    merged_df = pd.read_csv(output_merged_file, delimiter='^', encoding='latin1')\n",
    "    print(merged_df.head())\n",
    "else:\n",
    "    print(f\"Error: The file at path {output_merged_file} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0c451-50fb-4055-ad86-01412379f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6680869-4a0a-424a-949c-2d041a977474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
