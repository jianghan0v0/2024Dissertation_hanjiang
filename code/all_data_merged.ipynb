{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "422788bc-b3f1-4403-9c53-68b80842018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded nfirs_fire_hazmat_pdr_2022.zip\n",
      "Extracted files to nfirs_fire_hazmat_pdr_2022\n",
      "Extracted files to nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623\n",
      "NFIRS_FIRES_2022_102623/\n",
      "    incidentaddress.txt\n",
      "    merged_data.csv\n",
      "    fireincident.txt\n",
      "    merged_casualty_data.csv\n",
      "    civiliancasualty.txt\n",
      "    hazchem.txt\n",
      "    arson.txt\n",
      "    basicincident.txt\n",
      "    hazmat.txt\n",
      "    wildlands.txt\n",
      "    arsonjuvsub.txt\n",
      "    ems.txt\n",
      "    hazmatequipinvolved.txt\n",
      "    codelookup.txt\n",
      "    ffcasualty.txt\n",
      "    fdheader.txt\n",
      "    hazmobprop.txt\n",
      "    basicaid.txt\n",
      "    ffequipfail.txt\n",
      "    filtered_basicincident.csv\n",
      "    count_civiliancasualty.txt\n",
      "    arsonagencyreferal.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "# 抑制 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 远程文件的 URL\n",
    "url = 'https://fema.gov/about/reports-and-data/openfema/nfirs_fire_hazmat_pdr_2022.zip'\n",
    "\n",
    "# 本地保存的文件名\n",
    "local_zip_file = 'nfirs_fire_hazmat_pdr_2022.zip'\n",
    "\n",
    "# 第一级解压缩后的文件夹名\n",
    "first_extracted_folder = 'nfirs_fire_hazmat_pdr_2022'\n",
    "\n",
    "# 第二级解压缩后的文件夹名\n",
    "second_extracted_folder = os.path.join(first_extracted_folder, 'NFIRS_FIRES_2022_102623')\n",
    "\n",
    "# 下载文件\n",
    "response = requests.get(url)\n",
    "with open(local_zip_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print(f\"Downloaded {local_zip_file}\")\n",
    "\n",
    "# 解压缩第一级文件\n",
    "with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(first_extracted_folder)\n",
    "print(f\"Extracted files to {first_extracted_folder}\")\n",
    "\n",
    "# 解压缩第二级文件\n",
    "second_zip_file = os.path.join(first_extracted_folder, 'nfirs_fire_hazmat_pdr_2022', 'NFIRS_FIRES_2022_102623.zip')\n",
    "with zipfile.ZipFile(second_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(second_extracted_folder)\n",
    "print(f\"Extracted files to {second_extracted_folder}\")\n",
    "\n",
    "# 列出解压后的目录结构\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "list_files(second_extracted_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b383ca-3602-4feb-8d9c-9169381fd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动指定每一列的数据类型\n",
    "dtype_civiliancasualty = {\n",
    "    'INCIDENT_KEY': 'object',\n",
    "    'STATE': 'object',\n",
    "    'FDID': 'object',\n",
    "    'INC_DATE': 'object',\n",
    "    'INC_NO': 'object',\n",
    "    'EXP_NO': 'float64',  # 将 int64 改为 float64 以允许 NA 值\n",
    "    'SEQ_NUMBER': 'float64', \n",
    "    'VERSION': 'object',\n",
    "    'GENDER': 'object',\n",
    "    'AGE': 'float64',\n",
    "    'RACE': 'object',\n",
    "    'ETHNICITY': 'object',\n",
    "    'AFFILIAT': 'object',\n",
    "    'INJ_DT_TIM': 'object',\n",
    "    'SEV': 'object',\n",
    "    'CAUSE_INJ': 'object',\n",
    "    'HUM_FACT1': 'object',\n",
    "    'HUM_FACT2': 'object',\n",
    "    'HUM_FACT3': 'object',\n",
    "    'HUM_FACT4': 'object',\n",
    "    'HUM_FACT5': 'object',\n",
    "    'HUM_FACT6': 'object',\n",
    "    'HUM_FACT7': 'object',\n",
    "    'HUM_FACT8': 'object',\n",
    "    'FACT_INJ1': 'object',\n",
    "    'FACT_INJ2': 'object',\n",
    "    'FACT_INJ3': 'object',\n",
    "    'ACTIV_INJ': 'object',\n",
    "    'LOC_INC': 'object',\n",
    "    'GEN_LOC_IN': 'object',\n",
    "    'STORY_INC': 'object',\n",
    "    'STORY_INJ': 'object',\n",
    "    'SPC_LOC_IN': 'object',\n",
    "    'PRIM_SYMP': 'object',\n",
    "    'BODY_PART': 'object',\n",
    "    'CC_DISPOS': 'object'\n",
    "}\n",
    "\n",
    "dtype_fireincident = {\n",
    "    'INCIDENT_KEY': 'object',\n",
    "    'STATE': 'object',\n",
    "    'FDID': 'object',\n",
    "    'INC_DATE': 'object',\n",
    "    'INC_NO': 'object',\n",
    "    'EXP_NO': 'float64',\n",
    "    'VERSION': 'object',\n",
    "    'NUM_UNIT': 'float64',\n",
    "    'NOT_RES': 'object',\n",
    "    'BLDG_INVOL': 'float64',\n",
    "    'ACRES_BURN': 'float64',\n",
    "    'LESS_1ACRE': 'object',\n",
    "    'ON_SITE_M1': 'object',\n",
    "    'MAT_STOR1': 'object',\n",
    "    'ON_SITE_M2': 'object',\n",
    "    'MAT_STOR2': 'object',\n",
    "    'ON_SITE_M3': 'object',\n",
    "    'MAT_STOR3': 'object',\n",
    "    'AREA_ORIG': 'object',\n",
    "    'HEAT_SOURC': 'object',\n",
    "    'FIRST_IGN': 'object',\n",
    "    'CONF_ORIG': 'object',\n",
    "    'TYPE_MAT': 'object',\n",
    "    'CAUSE_IGN': 'object',\n",
    "    'FACT_IGN_1': 'object',\n",
    "    'FACT_IGN_2': 'object',\n",
    "    'HUM_FAC_1': 'object',\n",
    "    'HUM_FAC_2': 'object',\n",
    "    'HUM_FAC_3': 'object',\n",
    "    'HUM_FAC_4': 'object',\n",
    "    'HUM_FAC_5': 'object',\n",
    "    'HUM_FAC_6': 'object',\n",
    "    'HUM_FAC_7': 'object',\n",
    "    'HUM_FAC_8': 'object',\n",
    "    'AGE': 'object',\n",
    "    'SEX': 'object',\n",
    "    'EQUIP_INV': 'object',\n",
    "    'SUP_FAC_1': 'object',\n",
    "    'SUP_FAC_2': 'object',\n",
    "    'SUP_FAC_3': 'object',\n",
    "    'MOB_INVOL': 'object',\n",
    "    'MOB_TYPE': 'object',\n",
    "    'MOB_MAKE': 'object',\n",
    "    'MOB_MODEL': 'object',\n",
    "    'MOB_YEAR': 'object',\n",
    "    'MOB_LIC_PL': 'object',\n",
    "    'MOB_STATE': 'object',\n",
    "    'MOB_VIN_NO': 'object',\n",
    "    'EQ_BRAND': 'object',\n",
    "    'EQ_MODEL': 'object',\n",
    "    'EQ_SER_NO': 'object',\n",
    "    'EQ_YEAR': 'object',\n",
    "    'EQ_POWER': 'object',\n",
    "    'EQ_PORT': 'object',\n",
    "    'FIRE_SPRD': 'object',\n",
    "    'STRUC_TYPE': 'object',\n",
    "    'STRUC_STAT': 'object',\n",
    "    'BLDG_ABOVE': 'object',\n",
    "    'BLDG_BELOW': 'object',\n",
    "    'BLDG_LGTH': 'object',\n",
    "    'BLDG_WIDTH': 'object',\n",
    "    'TOT_SQ_FT': 'object',\n",
    "    'FIRE_ORIG': 'object',\n",
    "    'ST_DAM_MIN': 'object',\n",
    "    'ST_DAM_SIG': 'object',\n",
    "    'ST_DAM_HVY': 'object',\n",
    "    'ST_DAM_XTR': 'object',\n",
    "    'FLAME_SPRD': 'object',\n",
    "    'ITEM_SPRD': 'object',\n",
    "    'MAT_SPRD': 'object',\n",
    "    'DETECTOR': 'object',\n",
    "    'DET_TYPE': 'object',\n",
    "    'DET_POWER': 'object',\n",
    "    'DET_OPERAT': 'object',\n",
    "    'DET_EFFECT': 'object',\n",
    "    'DET_FAIL': 'object',\n",
    "    'AES_PRES': 'object',\n",
    "    'AES_TYPE': 'object',\n",
    "    'AES_OPER': 'object',\n",
    "    'NO_SPR_OP': 'object',\n",
    "    'AES_FAIL': 'object'\n",
    "}\n",
    "\n",
    "dtype_basicincident = {\n",
    "    'INCIDENT_KEY': 'object',\n",
    "    'STATE': 'object',\n",
    "    'FDID': 'object',\n",
    "    'INC_DATE': 'object',\n",
    "    'INC_NO': 'object',\n",
    "    'EXP_NO': 'float64',\n",
    "    'VERSION': 'object',\n",
    "    'DEPT_STA': 'object',\n",
    "    'INC_TYPE': 'object',\n",
    "    'ADD_WILD': 'object',\n",
    "    'AID': 'object',\n",
    "    'ALARM': 'object',\n",
    "    'ARRIVAL': 'object',\n",
    "    'INC_CONT': 'object',\n",
    "    'LU_CLEAR': 'object',\n",
    "    'SHIFT': 'object',\n",
    "    'ALARMS': 'object',\n",
    "    'DISTRICT': 'object',\n",
    "    'ACT_TAK1': 'object',\n",
    "    'ACT_TAK2': 'object',\n",
    "    'ACT_TAK3': 'object',\n",
    "    'APP_MOD': 'object',\n",
    "    'SUP_APP': 'object',\n",
    "    'EMS_APP': 'object',\n",
    "    'OTH_APP': 'object',\n",
    "    'SUP_PER': 'object',\n",
    "    'EMS_PER': 'object',\n",
    "    'OTH_PER': 'object',\n",
    "    'RESOU_AID': 'object',\n",
    "    'PROP_LOSS': 'float64',\n",
    "    'CONT_LOSS': 'float64',\n",
    "    'PROP_VAL': 'float64',\n",
    "    'CONT_VAL': 'float64',\n",
    "    'FF_DEATH': 'float64', \n",
    "    'OTH_DEATH': 'float64',  # 将 int64 改为 float64 以允许 NA 值\n",
    "    'FF_INJ': 'float64',  # 将 int64 改为 float64 以允许 NA 值\n",
    "    'OTH_INJ': 'float64',\n",
    "    'DET_ALERT': 'object',\n",
    "    'HAZ_REL': 'object',\n",
    "    'MIXED_USE': 'object',\n",
    "    'PROP_USE': 'object',\n",
    "    'CENSUS': 'object'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c508b7d9-2c35-4154-8ad5-ce577eb16907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate CSV file saved to nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623/intermediate_civiliancasualty.csv\n",
      "Downloaded nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623/intermediate_civiliancasualty.csv to /home/jovyan/work/004DISSERTATION/downloads/intermediate_civiliancasualty.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取并保存 civiliancasualty.txt 为 CSV 文件\n",
    "civiliancasualty_path = os.path.join(second_extracted_folder, 'civiliancasualty.txt')\n",
    "intermediate_csv_path = os.path.join(second_extracted_folder, 'intermediate_civiliancasualty.csv')\n",
    "\n",
    "if os.path.exists(civiliancasualty_path):\n",
    "    try:\n",
    "        # 使用 pandas 读取 civiliancasualty.txt 文件\n",
    "        civiliancasualty_df = pd.read_csv(civiliancasualty_path, delimiter='^', encoding='latin1')\n",
    "        \n",
    "        # 保存为中间 CSV 文件\n",
    "        civiliancasualty_df.to_csv(intermediate_csv_path, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Intermediate CSV file saved to {intermediate_csv_path}\")\n",
    "        \n",
    "        # 确保目标目录存在\n",
    "        output_dir = os.path.join(os.getcwd(), 'downloads')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 下载文件到工作目录的 downloads 目录\n",
    "        local_download_path = os.path.join(output_dir, 'intermediate_civiliancasualty.csv')\n",
    "        shutil.copy(intermediate_csv_path, local_download_path)\n",
    "        print(f\"Downloaded {intermediate_csv_path} to {local_download_path}\")\n",
    "        \n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading the file with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file at path {civiliancasualty_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa628e57-6321-4b06-b074-29e773a0b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate CSV file saved to nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623/intermediate_civiliancasualty.csv\n",
      "Generated nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623/full_civiliancasualty.csv\n",
      "Downloaded nfirs_fire_hazmat_pdr_2022/NFIRS_FIRES_2022_102623/full_civiliancasualty.csv to /home/jovyan/work/004DISSERTATION/downloads/full_civiliancasualty.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# 读取并保存 civiliancasualty.txt 为 CSV 文件\n",
    "civiliancasualty_path = os.path.join(second_extracted_folder, 'civiliancasualty.txt')\n",
    "intermediate_csv_path = os.path.join(second_extracted_folder, 'intermediate_civiliancasualty.csv')\n",
    "\n",
    "if os.path.exists(civiliancasualty_path):\n",
    "    try:\n",
    "        # 使用 pandas 读取 civiliancasualty.txt 文件\n",
    "        civiliancasualty_df = pd.read_csv(civiliancasualty_path, delimiter='^', encoding='latin1')\n",
    "        \n",
    "        # 保存为中间 CSV 文件\n",
    "        civiliancasualty_df.to_csv(intermediate_csv_path, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Intermediate CSV file saved to {intermediate_csv_path}\")\n",
    "        \n",
    "        # 使用 dask 读取中间 CSV 文件\n",
    "        dask_civiliancasualty_df = dd.read_csv(intermediate_csv_path, delimiter='^', encoding='latin1', dtype=dtype_civiliancasualty)\n",
    "        \n",
    "        # 计算每个事件的伤亡人数\n",
    "        civiliancasualty_count = dask_civiliancasualty_df.groupby('INCIDENT_KEY').size().reset_index()\n",
    "        civiliancasualty_count.columns = ['INCIDENT_KEY', 'civil_num']\n",
    "        \n",
    "        # 使用 dask 计算结果进行合并\n",
    "        merged_civiliancasualty = dd.merge(dask_civiliancasualty_df, civiliancasualty_count, on='INCIDENT_KEY', how='left')\n",
    "        \n",
    "        # 保存完整的文件\n",
    "        output_full_file = os.path.join(second_extracted_folder, 'full_civiliancasualty.csv')\n",
    "        merged_civiliancasualty.compute().to_csv(output_full_file, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Generated {output_full_file}\")\n",
    "        \n",
    "        # 确保目标目录存在\n",
    "        output_dir = os.path.join(os.getcwd(), 'downloads')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 下载文件到工作目录的 downloads 目录\n",
    "        local_download_path = os.path.join(output_dir, 'full_civiliancasualty.csv')\n",
    "        shutil.copy(output_full_file, local_download_path)\n",
    "        print(f\"Downloaded {output_full_file} to {local_download_path}\")\n",
    "        \n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading the file with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file at path {civiliancasualty_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625e19-611b-4b6e-a011-038465964423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c54cb1-4736-410b-b1ef-c5816a1d777f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `drop_by_shallow_copy`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nKeyError(\"['STATE', 'FDID', 'INC_DATE', 'INC_NO', 'EXP_NO', 'VERSION'] not found in axis\")\n\nTraceback:\n---------\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6897, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 772, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\", line 5347, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\", line 4711, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\", line 4753, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6992, in drop\n    raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py:193\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6897\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[0;32m-> 6897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py:772\u001b[0m, in \u001b[0;36mdrop_by_shallow_copy\u001b[0;34m(df, columns, errors)\u001b[0m\n\u001b[1;32m    771\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [columns]\n\u001b[0;32m--> 772\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:5347\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5211\u001b[0m \u001b[38;5;124;03mDrop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5212\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5345\u001b[0m \u001b[38;5;124;03m        weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 5347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5349\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4754\u001b[0m indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6992\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6993\u001b[0m indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['STATE', 'FDID', 'INC_DATE', 'INC_NO', 'EXP_NO', 'VERSION'] not found in axis\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 删除重复的列\u001b[39;00m\n\u001b[1;32m     16\u001b[0m basicincident_columns_to_drop \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m basicincident_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m fireincident_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINCIDENT_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasicincident_columns_to_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 修改位置：使用 how='inner' 实现交集合并\u001b[39;00m\n\u001b[1;32m     20\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mmerge(merged_df, count_civiliancasualty_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINCIDENT_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:5615\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, columns, errors)\u001b[0m\n\u001b[1;32m   5612\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_axis(axis)\n\u001b[1;32m   5613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5614\u001b[0m     \u001b[38;5;66;03m# Columns must be specified if axis==0\u001b[39;00m\n\u001b[0;32m-> 5615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_by_shallow_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   5617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5618\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   5619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[1;32m   5620\u001b[0m         drop_by_shallow_copy, labels, errors\u001b[38;5;241m=\u001b[39merrors, enforce_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5621\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:1048\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6967\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6960\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   6961\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want the partitions to be aligned, and are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6962\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `map_partitions` directly, pass `align_dataframes=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6963\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   6965\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, _Frame)]\n\u001b[0;32m-> 6967\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43m_get_meta_map_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Scalar) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[1;32m   6969\u001b[0m     layer \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   6970\u001b[0m         (name, \u001b[38;5;241m0\u001b[39m): (\n\u001b[1;32m   6971\u001b[0m             apply,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6975\u001b[0m         )\n\u001b[1;32m   6976\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:7078\u001b[0m, in \u001b[0;36m_get_meta_map_partitions\u001b[0;34m(args, dfs, func, kwargs, meta, parent_meta)\u001b[0m\n\u001b[1;32m   7074\u001b[0m     parent_meta \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   7075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[1;32m   7076\u001b[0m     \u001b[38;5;66;03m# Use non-normalized kwargs here, as we want the real values (not\u001b[39;00m\n\u001b[1;32m   7077\u001b[0m     \u001b[38;5;66;03m# delayed values)\u001b[39;00m\n\u001b[0;32m-> 7078\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43m_emulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7079\u001b[0m     meta_is_emulated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   7080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py:6896\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_emulate\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   6892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6893\u001b[0m \u001b[38;5;124;03m    Apply a function using args / kwargs. If arguments contain dd.DataFrame /\u001b[39;00m\n\u001b[1;32m   6894\u001b[0m \u001b[38;5;124;03m    dd.Series, using internal cache (``_meta``) for calculation\u001b[39;00m\n\u001b[1;32m   6895\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6896\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraise_on_meta_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuncname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mudf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_numeric_only_deprecation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   6897\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py:214\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    205\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    213\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata inference failed in `drop_by_shallow_copy`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nKeyError(\"['STATE', 'FDID', 'INC_DATE', 'INC_NO', 'EXP_NO', 'VERSION'] not found in axis\")\n\nTraceback:\n---------\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/core.py\", line 6897, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/dask/dataframe/utils.py\", line 772, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\", line 5347, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\", line 4711, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\", line 4753, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6992, in drop\n    raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n"
     ]
    }
   ],
   "source": [
    "# 合并 count_civiliancasualty.txt, fireincident.txt, basicincident.txt 文件\n",
    "fireincident_path = os.path.join(second_extracted_folder, 'fireincident.txt')\n",
    "basicincident_path = os.path.join(second_extracted_folder, 'basicincident.txt')\n",
    "output_merged_file = os.path.join(second_extracted_folder, 'merged_data.csv')\n",
    "\n",
    "if os.path.exists(fireincident_path) and os.path.exists(basicincident_path):\n",
    "    try:\n",
    "        fireincident_df = dd.read_csv(fireincident_path, delimiter='^', encoding='latin1', dtype=dtype_fireincident)\n",
    "        basicincident_df = dd.read_csv(basicincident_path, delimiter='^', encoding='latin1', dtype=dtype_basicincident)\n",
    "        count_civiliancasualty_df = dd.read_csv(output_count_civiliancasualty, delimiter='^', encoding='latin1')\n",
    "\n",
    "        # 修改位置：使用 how='inner' 实现交集合并\n",
    "        merged_df = dd.merge(fireincident_df, basicincident_df, on='INCIDENT_KEY', how='inner')\n",
    "\n",
    "        # 删除重复的列\n",
    "        basicincident_columns_to_drop = [col for col in basicincident_df.columns if col in fireincident_df.columns and col != 'INCIDENT_KEY']\n",
    "        merged_df = merged_df.drop(columns=basicincident_columns_to_drop)\n",
    "\n",
    "        # 修改位置：使用 how='inner' 实现交集合并\n",
    "        merged_df = dd.merge(merged_df, count_civiliancasualty_df, on='INCIDENT_KEY', how='inner')\n",
    "\n",
    "        merged_df.to_csv(output_merged_file, single_file=True, sep='^', encoding='latin1', index=False)\n",
    "        print(f\"Merged data saved to {output_merged_file}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading one of the files with 'latin1' encoding: {e}\")\n",
    "else:\n",
    "    print(\"Error: One of the required files does not exist.\")\n",
    "\n",
    "# 显示合并后的数据\n",
    "if os.path.exists(output_merged_file):\n",
    "    # 使用 low_memory=False 以避免 DtypeWarning\n",
    "    merged_df = pd.read_csv(output_merged_file, delimiter='^', encoding='latin1', low_memory=False)\n",
    "    print(merged_df.head())\n",
    "else:\n",
    "    print(f\"Error: The file at path {output_merged_file} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b32b1-3f55-4031-8128-670e09c46b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ab08f-0666-4da9-8172-1a5bd12cec20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
